{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load and clean the data. Filter out the \" at the end and beginning of each motivation and add a start and end token. Also filter out the <i></i> that occur in some motivations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_csv = pd.read_csv('data/winner_train.csv')\n",
    "motivations = full_csv['motivation']\n",
    "motivations = motivations.dropna()\n",
    "\n",
    "\n",
    "def prepare_data(data_frame):\n",
    "    data_frame = data_frame.reset_index(drop=True)\n",
    "    for i, motivation in enumerate(data_frame):\n",
    "        # Remove leading and trailing whitespace and double quotes\n",
    "        motivation = motivation.strip().strip('\"')\n",
    "\n",
    "        # Remove <i> and </i> tags\n",
    "        motivation = motivation.replace('<i>', '').replace('</i>', '')\n",
    "        \n",
    "        # Convert to lowercase and add start/end tokens\n",
    "        motivation = '#START# ' + motivation.lower() + ' #END#' \n",
    "        \n",
    "        # Update the motivations list with the modified motivation string\n",
    "        data_frame[i] = motivation\n",
    "\n",
    "    # Remove duplicates\n",
    "    data_frame = data_frame.drop_duplicates()\n",
    "        \n",
    "    return data_frame\n",
    "\n",
    "\n",
    "motivations = prepare_data(motivations)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count probability of each word\n",
    "def get_word_count_dict(df):\n",
    "    word_count = {}\n",
    "    for row in df:\n",
    "        for word in row.split():\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "    return word_count\n",
    "\n",
    "\n",
    "def probability_of_word(word, df):\n",
    "    word_count = get_word_count_dict(df)\n",
    "    return word_count[word] / sum(word_count.values())\n",
    "\n",
    "\n",
    "def bigram_counter(df):\n",
    "    bigram_count = {}\n",
    "    for row in df:\n",
    "        words = row.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = (words[i], words[i+1])\n",
    "            if bigram in bigram_count:\n",
    "                bigram_count[bigram] += 1\n",
    "            else:\n",
    "                bigram_count[bigram] = 1\n",
    "    return bigram_count\n",
    "\n",
    "\n",
    "def sort_dictionary(dictionary):\n",
    "    sorted_dict = dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=True))\n",
    "    return sorted_dict\n",
    "\n",
    "\n",
    "def trigram_counter(df):\n",
    "    trigram_count = {}\n",
    "    for row in df:\n",
    "        words = row.split()\n",
    "        for i in range(len(words) - 2):\n",
    "            trigram = (words[i], words[i+1], words[i+2])\n",
    "            if trigram in trigram_count:\n",
    "                trigram_count[trigram] += 1\n",
    "            else:\n",
    "                trigram_count[trigram] = 1\n",
    "    return trigram_count\n",
    "\n",
    "\n",
    "def probability_of_sentence_bigram(sentence, df):\n",
    "    bigram_count = bigram_counter(df)\n",
    "    word_count = get_word_count_dict(df)\n",
    "    words = sentence.split()\n",
    "    probability = 1\n",
    "\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = (words[i], words[i+1])\n",
    "        # Divide by number of times the first word appears (and can be followed by another word)\n",
    "        if bigram in bigram_count:\n",
    "            to_mult = bigram_count[bigram] / word_count[words[i]]\n",
    "        else:\n",
    "            to_mult = 0.00001\n",
    "            \n",
    "         # Apply repetition penalty\n",
    "        for j, word in enumerate(words):\n",
    "            if (word == words[i] and i !=j):\n",
    "                to_mult *= 0.000001\n",
    "     \n",
    "        probability *= to_mult\n",
    "    return probability\n",
    "\n",
    "\n",
    "def probability_of_sentence_trigram(sentence, df):\n",
    "    trigram_count = trigram_counter(df)\n",
    "    word_count = get_word_count_dict(df)\n",
    "    words = sentence.split()\n",
    "    probability = 1\n",
    "\n",
    "    for i in range(len(words) - 2):\n",
    "        trigram = (words[i], words[i+1], words[i+2])\n",
    "        # Divide by number of times the first word appears (and can be followed by another word)\n",
    "        if trigram in trigram_count:\n",
    "            to_mult = trigram_count[trigram] / word_count[words[i]]\n",
    "        else:\n",
    "            to_mult = 0.00001\n",
    "            \n",
    "         # Apply repetition penalty\n",
    "        for j, word in enumerate(words):\n",
    "            if (word == words[i] and i !=j):\n",
    "                to_mult *= 0.000001\n",
    "     \n",
    "        probability *= to_mult    \n",
    "    return probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Generation a random motivation\n",
    "import random\n",
    "def random_motivation(df):\n",
    "    bigram_count = bigram_counter(df)\n",
    "    words = []\n",
    "    word = '#START#'\n",
    "    while word != '#END#':\n",
    "        words.append(word)\n",
    "        next_word = random.choices(list(bigram_count.keys()), list(bigram_count.values()))\n",
    "        word = next_word[0][1]\n",
    "    return ' '.join(words[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: Generation of the most probable motivation using Bigram \n",
    "def most_probable_motivation(df, sorted = True):\n",
    "    bigram_count = bigram_counter(df)\n",
    "    if sorted:\n",
    "        bigram_count = sort_dictionary(bigram_count)\n",
    "    sentence = ''\n",
    "    word = '#START#'\n",
    "    sentence += (word) \n",
    "    while word != '#END#' :#and len(sentence.split(' ')) < 10:\n",
    "        all_bigrams_with_word = [bigram for bigram in bigram_count.keys() if bigram[0] == word]    \n",
    "        \n",
    "        if not sorted:\n",
    "            # take 15 random bigrams from possible_15_words\n",
    "            length = len(all_bigrams_with_word)\n",
    "            if length >= 15:\n",
    "                possible_15_words = random.sample(all_bigrams_with_word, 15)\n",
    "            else:\n",
    "                possible_15_words = random.sample(all_bigrams_with_word, length)\n",
    "        else:\n",
    "            possible_15_words = all_bigrams_with_word[:15]\n",
    "        \n",
    "        if len(possible_15_words) == 0:\n",
    "            most_common_words = sort_dictionary(dict(islice(\n",
    "                            get_word_count_dict(motivations).items(), 15)))\n",
    "            for common in most_common_words:\n",
    "                possible_15_words.append((word, common))\n",
    "        \n",
    "        possible_sentences = [sentence + ' ' + word[1] for word in possible_15_words]\n",
    "        probabilities = [probability_of_sentence_bigram(sentence, df) for sentence in possible_sentences]\n",
    "\n",
    "        \n",
    "        word = possible_15_words[probabilities.index(max(probabilities))][1]\n",
    "    \n",
    "        \n",
    "        sentence += ' ' + (word)\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random motivation chosen from top three "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3: Generating a probable motivation (introducing randomness), still using bigrams\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "def most_probable_motivation_with_random(df, sorted = True):\n",
    "    bigram_count = bigram_counter(df)\n",
    "    if sorted:\n",
    "        bigram_count = sort_dictionary(bigram_count)\n",
    "    sentence = ''\n",
    "    word = '#START#'\n",
    "    sentence += (word) \n",
    "    while word != '#END#':\n",
    "        all_bigrams_with_word = [bigram for bigram in bigram_count.keys() if bigram[0] == word]\n",
    "\n",
    "        if not sorted:\n",
    "            # take 15 random bigrams from possible_15_words\n",
    "            length = len(all_bigrams_with_word)\n",
    "            if length >= 15:\n",
    "                possible_15_words = random.sample(all_bigrams_with_word, 15)\n",
    "            else:\n",
    "                possible_15_words = random.sample(all_bigrams_with_word, length)\n",
    "        else:\n",
    "            possible_15_words = all_bigrams_with_word[:15]\n",
    "\n",
    "        if len(possible_15_words) == 0:\n",
    "            most_common_words = sort_dictionary(dict(islice(get_word_count_dict(motivations).items(), 15)))\n",
    "            for common in most_common_words:\n",
    "                possible_15_words.append((word, common))\n",
    "        \n",
    "        \n",
    "        word_and_prob = {}\n",
    "        for select in possible_15_words:\n",
    "            word_and_prob[select[1]] = probability_of_sentence_bigram(sentence + ' ' + select[1], df)\n",
    "\n",
    "        word_and_prob = sort_dictionary(word_and_prob)\n",
    "        # Some words don't have three options for their next word\n",
    "        lenght = len(word_and_prob)\n",
    "        lenght = (lenght if lenght < 3 else 3) \n",
    "        top_three_words = list(word_and_prob.keys())[:lenght]\n",
    "        word = np.random.choice(top_three_words)\n",
    "        sentence += ' ' + (word)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4: Generation of the most probable motivation using trigrams \n",
    "def use_bigram_instead(word, sentence, sorted, df):\n",
    "    bigram_count = bigram_counter(df)\n",
    "    if sorted:\n",
    "        bigram_count = sort_dictionary(bigram_count)\n",
    "    all_bigrams_with_word = [bigram for bigram in bigram_count.keys() if bigram[0] == word]\n",
    "\n",
    "    if not sorted:\n",
    "        # take 15 random bigrams from possible_15_words\n",
    "        length = len(all_bigrams_with_word)\n",
    "        if length >= 15:\n",
    "            possible_15_words = random.sample(all_bigrams_with_word, 15)\n",
    "        else:\n",
    "            possible_15_words = random.sample(all_bigrams_with_word, length)\n",
    "    else:\n",
    "        possible_15_words = all_bigrams_with_word[:15]\n",
    "\n",
    "    if len(possible_15_words) == 0:\n",
    "        most_common_words = sort_dictionary(dict(islice(get_word_count_dict(motivations).items(), 15)))\n",
    "        for common in most_common_words:\n",
    "            possible_15_words.append((word, common))\n",
    "\n",
    "    possible_sentences = [sentence + ' ' + word[1] for word in possible_15_words]\n",
    "    probabilities = [probability_of_sentence_bigram(sentence, df) for sentence in possible_sentences]\n",
    "\n",
    "    word = possible_15_words[probabilities.index(max(probabilities))][1]\n",
    "    return word\n",
    "\n",
    "def most_probable_motivation_trigram(df, sorted = True):\n",
    "    sorted_val = sorted\n",
    "    trigram_count = trigram_counter(df)\n",
    "    if sorted:\n",
    "        trigram_count = sort_dictionary(trigram_count)\n",
    "    sentence = ''\n",
    "    last_word = '#START#'\n",
    "    sentence += (last_word) \n",
    "    while last_word != '#END#' :\n",
    "        word_to_add = ''\n",
    "\n",
    "        if len(sentence.split(' ')) == 1 :\n",
    "            word_to_add = use_bigram_instead(last_word, sentence, sorted_val, df)\n",
    "\n",
    "        else:\n",
    "            second_to_last_word = sentence.split(' ')[-2]\n",
    "            all_trigrams_with_word = [trigram for trigram in trigram_count.keys() if trigram[1] == last_word and trigram[0] == second_to_last_word]\n",
    "            \n",
    "            if not sorted:\n",
    "                length = len(all_trigrams_with_word)\n",
    "                if length >= 15:\n",
    "                    possible_15_words = random.sample(all_trigrams_with_word, 15)\n",
    "                else:\n",
    "                    possible_15_words = random.sample(all_trigrams_with_word, length)\n",
    "            else:\n",
    "                possible_15_words = all_trigrams_with_word[:15]\n",
    "\n",
    "            if len(possible_15_words) == 0:\n",
    "                word_to_add = use_bigram_instead(last_word, sentence, sorted_val, df)\n",
    "                \n",
    "            else:\n",
    "                possible_sentences = [sentence + ' ' + word[2] for word in possible_15_words]\n",
    "                probabilities = [probability_of_sentence_trigram(sentence, df) for sentence in possible_sentences]\n",
    "                word_to_add = possible_15_words[probabilities.index(max(probabilities))][2]\n",
    "        \n",
    "        sentence += ' ' + (word_to_add)\n",
    "        last_word = word_to_add\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the category will also be considered. First set up the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframes for the specific categories\n",
    "df_chemistry = full_csv[full_csv['category'] == 'Chemistry']['motivation']\n",
    "df_chemistry = df_chemistry.dropna()\n",
    "df_chemistry = prepare_data(df_chemistry)\n",
    "\n",
    "df_literature = full_csv[full_csv['category'] == 'Literature']['motivation']\n",
    "df_literature = df_literature.dropna()\n",
    "df_literature = prepare_data(df_literature)\n",
    "\n",
    "df_peace = full_csv[full_csv['category'] == 'Peace']['motivation']\n",
    "df_peace = df_peace.dropna()\n",
    "df_peace = prepare_data(df_peace)\n",
    "\n",
    "df_literature = full_csv[full_csv['category'] == 'Literature']['motivation']\n",
    "df_literature = df_literature.dropna()\n",
    "df_literature = prepare_data(df_literature)\n",
    "\n",
    "df_physics = full_csv[full_csv['category'] == 'Physics']['motivation']\n",
    "df_physics = df_physics.dropna()\n",
    "df_physics = prepare_data(df_physics)\n",
    "\n",
    "df_medicine = full_csv[full_csv['category'] == 'Medicine']['motivation']\n",
    "df_medicine = df_medicine.dropna()\n",
    "df_medicine = prepare_data(df_medicine)\n",
    "\n",
    "df_economics = full_csv[full_csv['category'] == 'Economics']['motivation']\n",
    "df_economics = df_economics.dropna()\n",
    "df_economics = prepare_data(df_economics)\n",
    "\n",
    "df_general = full_csv['motivation']\n",
    "df_general = df_general.dropna()\n",
    "df_general = prepare_data(df_general)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up helper methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate most common words for each category that are not in general\n",
    "def get_top_15_words(word_count, word_count_general):\n",
    "    top_15_words = {}\n",
    "    for word in word_count:\n",
    "        top_15_words[word] = probability_of_word_in_category(word, word_count, word_count_general)\n",
    "    top_15_words = sort_dictionary(top_15_words)\n",
    "    return list(top_15_words.keys())[:15]\n",
    "\n",
    "def probability_of_word_in_category(word, word_count, word_count_general):\n",
    "    if word not in word_count:\n",
    "        return 0.000001\n",
    "    return max(word_count[word] / word_count_general[word], 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate most probable motivation for each category using bigrams\n",
    "def most_probable_motivation_for_category(df, df_category, sorted = True):\n",
    "    word_count_general = get_word_count_dict(df)\n",
    "    word_count_cat = get_word_count_dict(df_category)\n",
    "    common_words_for_cat = get_top_15_words(word_count_cat, word_count_general)  \n",
    "\n",
    "    bigram_count = bigram_counter(df)\n",
    "    if sorted:\n",
    "        bigram_count = sort_dictionary(bigram_count)\n",
    "    sentence = ''\n",
    "    word = '#START#'\n",
    "    sentence += (word) \n",
    "    while word != '#END#' :\n",
    "        possible_words = []\n",
    "        all_bigrams_with_word = [bigram for bigram in bigram_count.keys() if bigram[0] == word]    \n",
    "        \n",
    "        if not sorted:\n",
    "            length = len(all_bigrams_with_word)\n",
    "            if length >= 15:\n",
    "                possible_15_words = random.sample(all_bigrams_with_word, 15)\n",
    "            else:\n",
    "                possible_15_words = random.sample(all_bigrams_with_word, length)\n",
    "        else:\n",
    "            possible_15_words = all_bigrams_with_word[:15]\n",
    "        possible_words = possible_15_words\n",
    "   \n",
    "        #Add the most common words for the category to the possible words\n",
    "        for word in common_words_for_cat:      \n",
    "            possible_words.append((possible_15_words[0][0], word))\n",
    "\n",
    "        #create 30 possible sentences\n",
    "        possible_sentences = [sentence + ' ' + bigram[1] for bigram in possible_words]\n",
    "       \n",
    "        probabilities = []\n",
    "        for sent in possible_sentences:\n",
    "            prob = probability_of_sentence_bigram(sent, df) * probability_of_word_in_category(sent.split()[-1], word_count_cat, word_count_general)\n",
    "            probabilities.append(prob)\n",
    "        \n",
    "        word = possible_words[probabilities.index(max(probabilities))][1]\n",
    "       \n",
    "        sentence += ' ' + (word)\n",
    "    return sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation, first using the BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper method for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first_last_word(string):\n",
    "    words = string.split()\n",
    "    if len(words) > 2:\n",
    "        return ' '.join(words[1:-1])\n",
    "    else:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Take max, min and median/mean of the scores\n",
    "# All existing motivations each in list format\n",
    "eval_csv = pd.read_csv('data/winner_eval.csv')\n",
    "#choose column named motivation\n",
    "eval = eval_csv['motivation']\n",
    "eval = prepare_data(eval)\n",
    "motivation_split_list = [sentence.split()[1:-1] for sentence in eval]\n",
    "\n",
    "\n",
    "# Evaluating random motivation\n",
    "scores_random = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = random_motivation(motivations)\n",
    "    for i in range(len(motivation_split_list)):\n",
    "        score = sentence_bleu(motivation_split_list[i], candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        scores_random.append(score)\n",
    "    all_scores.append(np.mean(scores_random))\n",
    "scores_random.sort()\n",
    "\n",
    "\n",
    "# Evaluating most probable motivation, bigram, sorted\n",
    "scores_most_probable_bigram_sort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation(motivations, sorted = True)\n",
    "    for i in range(len(motivation_split_list)):\n",
    "        score = sentence_bleu(motivation_split_list[i], remove_first_last_word(candidate), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        scores_most_probable_bigram_sort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_bigram_sort))\n",
    "scores_most_probable_bigram_sort.sort()\n",
    "\n",
    "# Evaluating most probable motivation, bigram, unsorted\n",
    "scores_most_probable_bigram_NOsort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation(motivations, sorted = False)\n",
    "    for i in range(len(motivation_split_list)):\n",
    "        score = sentence_bleu(motivation_split_list[i], remove_first_last_word(candidate), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        scores_most_probable_bigram_NOsort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_bigram_NOsort))\n",
    "scores_most_probable_bigram_NOsort.sort()\n",
    "\n",
    "\n",
    "# Evaluating most probable motivation with randomness, sorted\n",
    "scores_most_probable_random_sort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation_with_random(motivations, sorted = True)\n",
    "    for i in range(len(motivation_split_list)):\n",
    "        score = sentence_bleu(motivation_split_list[i], remove_first_last_word(candidate), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        scores_most_probable_random_sort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_random_sort))\n",
    "scores_most_probable_random_sort.sort()\n",
    "\n",
    "\n",
    "\n",
    "# Evaluating most probable motivation with randomness, unsorted\n",
    "scores_most_probable_random_NOsort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation_with_random(motivations, sorted = False)\n",
    "    for i in range(len(motivation_split_list)):\n",
    "        score = sentence_bleu(motivation_split_list[i], remove_first_last_word(candidate), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        scores_most_probable_random_NOsort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_random_NOsort))\n",
    "scores_most_probable_random_NOsort.sort()\n",
    "\n",
    "\n",
    "# Evaluating most probable motivation with trigram, sorted\n",
    "scores_most_probable_trigram_sort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation_trigram(motivations, sorted = True)\n",
    "    for i in range(len(motivation_split_list)):\n",
    "        score = sentence_bleu(motivation_split_list[i], remove_first_last_word(candidate), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        scores_most_probable_trigram_sort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_trigram_sort))\n",
    "scores_most_probable_trigram_sort.sort()\n",
    "\n",
    "# Evaluating most probable motivation with trigram, unsorted\n",
    "scores_most_probable_trigram_NOsort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation_trigram(motivations, sorted = False)\n",
    "    for i in range(len(motivation_split_list)):\n",
    "        score = sentence_bleu(motivation_split_list[i], remove_first_last_word(candidate), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        scores_most_probable_trigram_NOsort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_trigram_NOsort))\n",
    "scores_most_probable_trigram_NOsort.sort()\n",
    "\n",
    "\n",
    "scores_category = []\n",
    "category_dfs = [df_chemistry, df_literature, df_peace, df_physics, df_medicine, df_economics]\n",
    "for df in category_dfs:\n",
    "    all_scores = []\n",
    "    for i in range(2):\n",
    "        candidate = most_probable_motivation_for_category(motivations, df, sorted = False)\n",
    "        for i in range(len(motivation_split_list)):\n",
    "            score = sentence_bleu(motivation_split_list[i], remove_first_last_word(candidate), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "            scores_category.append(score)      \n",
    "scores_category.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Random result* \n",
      " mean = 0.0919158751848741 \n",
      " Median = 0.08163579932055849 \n",
      " min = 0 \n",
      " max = 0.3244913214932798 \n",
      "\n",
      "*Most Probable result (Bigram, sorted)* \n",
      " mean = 0.19056844977054535 \n",
      " Median = 0.16851375741211452 \n",
      " min = 3.184491103071845e-78 \n",
      " max = 0.40611607179711245 \n",
      "\n",
      "*Most Probable result (Bigram, unsorted)* \n",
      " mean = 0.11679320360436268 \n",
      " Median = 0.10969605945035435 \n",
      " min = 1.6227569201872753e-78 \n",
      " max = 0.3484162270015532 \n",
      "\n",
      "*Most Probable with Random result (Bigram, sorted)* \n",
      " mean = 0.1354741903405869 \n",
      " Median = 0.10624821699408675 \n",
      " min = 8.759666304185185e-155 \n",
      " max = 0.6788877661016413 \n",
      "\n",
      "*Most Probable with Random result (Bigram, unsorted)* \n",
      " mean = 0.0598208544811542 \n",
      " Median = 0.04194807103827805 \n",
      " min = 7.310577634934788e-155 \n",
      " max = 0.31837754460503936 \n",
      "\n",
      "*Most Probable result (Trigram, sorted)* \n",
      " mean = 0.056006595919949324 \n",
      " Median = 0.05789421994462952 \n",
      " min = 1.7159929819832364e-78 \n",
      " max = 0.09979654358436284 \n",
      "\n",
      "*Most Probable result (Trigram, unsorted)* \n",
      " mean = 0.10458451909467668 \n",
      " Median = 0.09337389530621411 \n",
      " min = 1.7590674097092552e-78 \n",
      " max = 0.34487708925066407 \n",
      "\n",
      "*Most Probable result (Category and bigram, unsorted)* \n",
      " mean = 0.11754349262698177 \n",
      " Median = 0.09104521477898216 \n",
      " min = 1.6393179084177166e-78 \n",
      " max = 0.45875990763376584 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ranom generation\n",
    "print(\"*Random result*\", \"\\n\", \"mean =\", np.mean(scores_random),\"\\n\",\"Median =\", np.median(scores_random),\"\\n\",\"min =\", scores_random[0], \"\\n\", \"max =\", scores_random[-1], \"\\n\")\n",
    "\n",
    "# Most probable deterministic\n",
    "print(\"*Most Probable result (Bigram, sorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_bigram_sort),\"\\n\",\"Median =\", np.median(scores_most_probable_bigram_sort),\"\\n\",\"min =\", scores_most_probable_bigram_sort[0], \"\\n\", \"max =\", scores_most_probable_bigram_sort[-1], \"\\n\")\n",
    "\n",
    "# Most probable deterministic not sorted\n",
    "print(\"*Most Probable result (Bigram, unsorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_bigram_NOsort),\"\\n\",\"Median =\", np.median(scores_most_probable_bigram_NOsort),\"\\n\",\"min =\", scores_most_probable_bigram_NOsort[0], \"\\n\", \"max =\", scores_most_probable_bigram_NOsort[-1], \"\\n\")\n",
    "\n",
    "# Most probable with randomness\n",
    "print(\"*Most Probable with Random result (Bigram, sorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_random_sort),\"\\n\",\"Median =\", np.median(scores_most_probable_random_sort),\"\\n\",\"min =\", scores_most_probable_random_sort[0], \"\\n\", \"max =\", scores_most_probable_random_sort[-1], \"\\n\")\n",
    "\n",
    "# Most probable with randomness not sorted\n",
    "print(\"*Most Probable with Random result (Bigram, unsorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_random_NOsort),\"\\n\",\"Median =\", np.median(scores_most_probable_random_NOsort),\"\\n\",\"min =\", scores_most_probable_random_NOsort[0], \"\\n\", \"max =\", scores_most_probable_random_NOsort[-1], \"\\n\")\n",
    "\n",
    "# Most probable with trigram\n",
    "print(\"*Most Probable result (Trigram, sorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_trigram_sort),\"\\n\",\"Median =\", np.median(scores_most_probable_trigram_sort),\"\\n\",\"min =\", scores_most_probable_trigram_sort[0], \"\\n\", \"max =\", scores_most_probable_trigram_sort[-1], \"\\n\")\n",
    "\n",
    "# Most probable with trigram not sorted\n",
    "print(\"*Most Probable result (Trigram, unsorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_trigram_NOsort),\"\\n\",\"Median =\", np.median(scores_most_probable_trigram_NOsort),\"\\n\",\"min =\", scores_most_probable_trigram_NOsort[0], \"\\n\", \"max =\", scores_most_probable_trigram_NOsort[-1], \"\\n\")\n",
    "\n",
    "# Most probable with category\n",
    "print(\"*Most Probable result (Category and bigram, unsorted)*\", \"\\n\", \"mean =\", np.mean(scores_category),\"\\n\",\"Median =\", np.median(scores_category),\"\\n\",\"min =\", scores_category[0], \"\\n\", \"max =\", scores_category[-1], \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, evaluation using semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /Users/elin/anaconda3/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torch in /Users/elin/anaconda3/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: scipy in /Users/elin/anaconda3/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /Users/elin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.38.2)\n",
      "Requirement already satisfied: tqdm in /Users/elin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: numpy in /Users/elin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/elin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/elin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: Pillow in /Users/elin/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in /Users/elin/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/elin/anaconda3/lib/python3.11/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/elin/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/elin/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/elin/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/elin/anaconda3/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: requests in /Users/elin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/elin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/elin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/elin/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/elin/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/elin/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/elin/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/elin/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/elin/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/elin/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/elin/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/elin/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/elin/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/elin/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers torch scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use a sentence transformer since the motivations has to be embedded to use cosine similarity\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Use a sentence transformer since the motivations has to be embedded to use cosine similarity\n",
    "model_name = \"paraphrase-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "def semantic_similarity_score(sentence1, sentence2):\n",
    "    embeddings1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "    score = 1 - cosine(embeddings1, embeddings2)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_csv = pd.read_csv('winner_eval.csv')\n",
    "eval = eval_csv['motivation']\n",
    "eval = prepare_data(eval)\n",
    "motivations_list = eval[1:]\n",
    "\n",
    "# Evaluating random motivation\n",
    "scores_random = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = random_motivation(motivations)\n",
    "    for sentence in motivations_list:\n",
    "        score = semantic_similarity_score(remove_first_last_word(sentence), candidate)\n",
    "        scores_random.append(score)\n",
    "    all_scores.append(np.mean(scores_random))\n",
    "scores_random.sort()\n",
    "\n",
    "\n",
    "# Evaluating most probable motivation, bigram, sorted\n",
    "scores_most_probable_bigram_sort = []\n",
    "for i in range(1):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation(motivations, sorted = True)\n",
    "    for sentence in motivations_list:\n",
    "        score = semantic_similarity_score(remove_first_last_word(sentence), remove_first_last_word(candidate))\n",
    "        scores_most_probable_bigram_sort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_bigram_sort))\n",
    "scores_most_probable_bigram_sort.sort()\n",
    "\n",
    "\n",
    "# Evaluating most probable motivation, bigram, unsorted\n",
    "scores_most_probable_bigram_NOsort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation(motivations, sorted = False)\n",
    "    for sentence in motivations_list:\n",
    "        score = semantic_similarity_score(remove_first_last_word(sentence), remove_first_last_word(candidate))\n",
    "        scores_most_probable_bigram_NOsort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_bigram_NOsort))\n",
    "scores_most_probable_bigram_NOsort.sort()\n",
    "\n",
    "\n",
    "# Evaluating most probable motivation with randomness, sorted\n",
    "scores_most_probable_random_sort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation_with_random(motivations, sorted = True)\n",
    "    for sentence in motivations_list:\n",
    "        score = semantic_similarity_score(remove_first_last_word(sentence), remove_first_last_word(candidate))\n",
    "        scores_most_probable_random_sort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_random_sort))\n",
    "scores_most_probable_random_sort.sort()\n",
    "\n",
    "\n",
    "# Evaluating most probable motivation with randomness, unsorted\n",
    "scores_most_probable_random_NOsort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation_with_random(motivations, sorted = False)\n",
    "    for sentence in motivations_list:\n",
    "        score = semantic_similarity_score(remove_first_last_word(sentence), remove_first_last_word(candidate))\n",
    "        scores_most_probable_random_NOsort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_random_NOsort))\n",
    "scores_most_probable_random_NOsort.sort()\n",
    "\n",
    "\n",
    "# Evaluating most probable motivation with trigram, sorted\n",
    "scores_most_probable_trigram_sort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation_trigram(motivations, sorted = True)\n",
    "    for sentence in motivations_list:\n",
    "        score = semantic_similarity_score(remove_first_last_word(sentence), remove_first_last_word(candidate))\n",
    "        scores_most_probable_trigram_sort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_trigram_sort))\n",
    "scores_most_probable_trigram_sort.sort()\n",
    "\n",
    "\n",
    "# Evaluating most probable motivation with trigram, unsorted\n",
    "scores_most_probable_trigram_NOsort = []\n",
    "for i in range(10):\n",
    "    all_scores = []\n",
    "    candidate = most_probable_motivation_trigram(motivations, sorted = False)\n",
    "    for sentence in motivations_list:\n",
    "        score = semantic_similarity_score(remove_first_last_word(sentence), remove_first_last_word(candidate))\n",
    "        scores_most_probable_trigram_NOsort.append(score)\n",
    "    all_scores.append(np.mean(scores_most_probable_trigram_NOsort))\n",
    "scores_most_probable_trigram_NOsort.sort()\n",
    "\n",
    "# Evaluating most probable motivation with category\n",
    "scores_category = []\n",
    "category_dfs = [df_chemistry, df_literature, df_peace, df_physics, df_medicine, df_economics]\n",
    "for df in category_dfs:\n",
    "    all_scores = []\n",
    "    for i in range(2):\n",
    "        candidate = most_probable_motivation_for_category(motivations, df, sorted = False)\n",
    "        for sentence in motivations_list:\n",
    "            score = semantic_similarity_score(remove_first_last_word(sentence), remove_first_last_word(candidate))\n",
    "            scores_category.append(score)\n",
    "scores_category.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Random result* \n",
      " mean = 0.18020729031413793 \n",
      " Median = 0.17391572892665863 \n",
      " min = -0.034190766513347626 \n",
      " max = 0.44707831740379333 \n",
      "\n",
      "*Most Probable result (Bigram, sorted)* \n",
      " mean = 0.30634115636348724 \n",
      " Median = 0.3047269582748413 \n",
      " min = 0.18095824122428894 \n",
      " max = 0.45845159888267517 \n",
      "\n",
      "*Most Probable result (Bigram, unsorted)* \n",
      " mean = 0.19249040284194052 \n",
      " Median = 0.18608178943395615 \n",
      " min = -0.009160984307527542 \n",
      " max = 0.4328368008136749 \n",
      "\n",
      "*Most Probable with Random result (Bigram, sorted)* \n",
      " mean = 0.21444718047976494 \n",
      " Median = 0.2089853212237358 \n",
      " min = 0.01768127828836441 \n",
      " max = 0.4470449388027191 \n",
      "\n",
      "*Most Probable with Random result (Bigram, unsorted)* \n",
      " mean = 0.20356779375113546 \n",
      " Median = 0.19679010659456253 \n",
      " min = -0.04162440448999405 \n",
      " max = 0.5702179074287415 \n",
      "\n",
      "*Most Probable result (Trigram, sorted)* \n",
      " mean = 0.20026683062314987 \n",
      " Median = 0.1787073016166687 \n",
      " min = 0.0983252003788948 \n",
      " max = 0.3128131628036499 \n",
      "\n",
      "*Most Probable result (Trigram, unsorted)* \n",
      " mean = 0.1856706516072154 \n",
      " Median = 0.14142177999019623 \n",
      " min = -0.04128126800060272 \n",
      " max = 0.5066378116607666 \n",
      "\n",
      "*Most Probable result (Category and bigram, unsorted)* \n",
      " mean = 0.15658450785364644 \n",
      " Median = 0.11879712343215942 \n",
      " min = -0.0935414731502533 \n",
      " max = 0.5623601078987122 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ranom generation\n",
    "print(\"*Random result*\", \"\\n\", \"mean =\", np.mean(scores_random),\"\\n\",\"Median =\", np.median(scores_random),\"\\n\",\"min =\", scores_random[0], \"\\n\", \"max =\", scores_random[-1], \"\\n\")\n",
    "\n",
    "# Most probable deterministic\n",
    "print(\"*Most Probable result (Bigram, sorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_bigram_sort),\"\\n\",\"Median =\", np.median(scores_most_probable_bigram_sort),\"\\n\",\"min =\", scores_most_probable_bigram_sort[0], \"\\n\", \"max =\", scores_most_probable_bigram_sort[-1], \"\\n\")\n",
    "\n",
    "# Most probable deterministic not sorted\n",
    "print(\"*Most Probable result (Bigram, unsorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_bigram_NOsort),\"\\n\",\"Median =\", np.median(scores_most_probable_bigram_NOsort),\"\\n\",\"min =\", scores_most_probable_bigram_NOsort[0], \"\\n\", \"max =\", scores_most_probable_bigram_NOsort[-1], \"\\n\")\n",
    "\n",
    "# Most probable with randomness\n",
    "print(\"*Most Probable with Random result (Bigram, sorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_random_sort),\"\\n\",\"Median =\", np.median(scores_most_probable_random_sort),\"\\n\",\"min =\", scores_most_probable_random_sort[0], \"\\n\", \"max =\", scores_most_probable_random_sort[-1], \"\\n\")\n",
    "\n",
    "# Most probable with randomness not sorted\n",
    "print(\"*Most Probable with Random result (Bigram, unsorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_random_NOsort),\"\\n\",\"Median =\", np.median(scores_most_probable_random_NOsort),\"\\n\",\"min =\", scores_most_probable_random_NOsort[0], \"\\n\", \"max =\", scores_most_probable_random_NOsort[-1], \"\\n\")\n",
    "\n",
    "# Most probable with trigram\n",
    "print(\"*Most Probable result (Trigram, sorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_trigram_sort),\"\\n\",\"Median =\", np.median(scores_most_probable_trigram_sort),\"\\n\",\"min =\", scores_most_probable_trigram_sort[0], \"\\n\", \"max =\", scores_most_probable_trigram_sort[-1], \"\\n\")\n",
    "\n",
    "# Most probable with trigram not sorted\n",
    "print(\"*Most Probable result (Trigram, unsorted)*\", \"\\n\", \"mean =\", np.mean(scores_most_probable_trigram_NOsort),\"\\n\",\"Median =\", np.median(scores_most_probable_trigram_NOsort),\"\\n\",\"min =\", scores_most_probable_trigram_NOsort[0], \"\\n\", \"max =\", scores_most_probable_trigram_NOsort[-1], \"\\n\")\n",
    "\n",
    "# Most probable with category\n",
    "print(\"*Most Probable result (Category and bigram, unsorted)*\", \"\\n\", \"mean =\", np.mean(scores_category),\"\\n\",\"Median =\", np.median(scores_category),\"\\n\",\"min =\", scores_category[0], \"\\n\", \"max =\", scores_category[-1], \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
